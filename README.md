# SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis

## Jailbreak Examples in Chemical Synthesis
In this section, we utilize the synthesis of TNT as a representative case to examine the effects of different prompting strategies on the attack GPT-4-o and Llama-3-70B-Instruct. By comparing these approaches, we highlight how varying prompts can influence the performance and vulnerability of each model under attack scenarios.

### Red-Team Prompting
<p align="center">
<img src="fig/red-team.png" alt="Red-Team Prompting" style="width:100%;"/>
</p>

### Explicit-Prompting
<p align="center">
<img src="fig/explicit.png" alt="Explicit-Prompting" style="width:100%;"/>
</p>

### Implicit-Prompting
<p align="center">
<img src="fig/implicit.png" alt="Implicit-Prompting" style="width:100%;"/>
</p>

### SMILES-Prompting
<p align="center">
<img src="fig/SMILES.png" alt="SMILES-Prompting" style="width:100%;"/>
</p>
